#AMBOT

































ğŸ§  AMBOT (RAG-Based Retrieval System )

ğŸ“¢ A powerful, scalable Retrieval-Augmented Generation (RAG) system using FAISS and DeepSeek LLM.

ğŸ›  Ideal for knowledge-based Q&A, document retrieval, and AI chatbots.

ğŸ’¡ Supports PDFs and web page processing with fast FAISS-based retrieval.

ğŸ“Œ Table of Contents

ğŸ“Œ Features

ğŸš€ Installation

ğŸ‘¤ Project Structure

ğŸ› ï¸ Usage

ğŸ”§ Configuration

âš¡ Performance Optimizations

ğŸ“Œ Features

âœ… Retrieval-Augmented Generation (RAG) â€“ Uses FAISS for context-aware query answering.

âœ… Multi-Document Support â€“ Processes PDFs and web pages.

âœ… DeepSeek-Style Prompting â€“ Ensures structured, fact-based responses.

âœ… Fast & Scalable Retrieval â€“ FAISS enables rapid similarity search.

âœ… Cosine Similarity Ranking â€“ Retrieves the most relevant chunks.

âœ… Cleans LLM Output â€“ Removes unwanted artifacts like <think>.

ğŸš€ Installation

1ï¸âƒ£ Clone the Repository

git clone https://github.com/NgFEP/Ambot.git

cd Ambot

2ï¸âƒ£ Install Dependencies

Ensure you have Python 3.8+ installed, then run:

pip install -r requirements.txt

If you don't have a requirements.txt, manually install:

pip install numpy faiss-cpu aiohttp PyPDF2 beautifulsoup4 ollama

3ï¸âƒ£ Set Up Directory Structure

Create the necessary folders:

mkdir -p embeddings data

ğŸ‘¤ Project Structure

ğŸ“¦ RAG_Project

â”œâ”€â”€ ğŸ“‚ embeddings/               # Stores generated embeddings

â”œâ”€â”€ ğŸ“‚ data/                     # Stores input PDF & web page files

â”œâ”€â”€ ğŸ“œ embedding_generation.py   # Generates embeddings from PDFs & HTML

â”œâ”€â”€ ğŸ“œ query_retrieval.py        # Retrieves and answers queries

â”œâ”€â”€ ğŸ“œ README.md                 # Documentation

ğŸ› ï¸ Usage

ğŸ‘‰ Step 1: Generate Embeddings

Run embedding_generation.py to process PDFs and web content:

python embedding_generation.py

This script:

Extracts text from PDFs and HTML.

Splits text into chunks.

Generates embeddings for each chunk.

Stores embeddings in FAISS for fast retrieval.

ğŸ‘‰ Step 2: Retrieve Answers

Run query_retrieval.py to query the stored knowledge base:

python query_retrieval.py

You'll be prompted to enter a query:

Enter your query (or type 'exit' to quit): What is the impact of inflation?

The system will:

1ï¸âƒ£ Compute the query embedding.

2ï¸âƒ£ Search FAISS for the most relevant chunks.

3ï¸âƒ£ Use DeepSeek LLM to generate an answer.

4ï¸âƒ£ Return a clean, structured response.

ğŸ”§ Configuration

Modify model settings in both scripts:

# Constants
EMBEDDINGS_DIR = "embeddings"  
DATA_FOLDER = "data"  
LLM_MODEL = "deepseek-r1:14b"  
EMBEDDING_MODEL = "nomic-embed-text"  

To change how many retrieved chunks are used for answering:

top_k = 3  # Change this to retrieve more or fewer chunks

âš¡ Performance Optimizations

âœ… Uses FAISS for fast similarity search.âœ… Async web scraping speeds up data extraction.âœ… Text chunking ensures better LLM processing.âœ… Cosine similarity ranking improves accuracy.


ğŸ’– If you like this project, give it a star! â­

